---
title: "Reproduced: Phonemic Similarity Metrics to Compare Pronunciation Methods"
author: "Kanishka Misra"
date: "December 23, 2018"
output:
  github_document: 
    df_print: kable
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Introduction

This document presents a replication of 'Phonemic Similarity Metrics to Compare Pronunciation Methods' by Hixon, Schneider, and Epstein (2011). The original article can be found [here]() <!--add link-->


## Loading Data

The CMU Pronunciation dictionary is the primary source of data in the study. At the time of the study, the authors used the CMU dict(v0.7a) with pronunciations of 133,354 words. Upon checking the [github repository of cmudict](https://github.com/Alexir/CMUdict), we only found a copy of v0.7a with 133311 words. Due to this discrepancy, we will be using the latest dictionary(v0.7b) with 133854 words and their corresponding pronunciations.

```{r}
library(tidyverse)
library(tidytext)
library(Rcpp)

sourceCpp(here::here("hixon-phonemic-similarity-metrics/rcpp/string_alignment.cpp"))

options(scipen = 99)

read_cmu <- function(url, sep = "\\s\\s") {
  result <- read_lines(url) %>%
    keep(!str_detect(., ";;;") & !(. == "")) %>%
    as_tibble() %>%
    separate(value, into = c("word", "pronunciation"), sep = sep, extra = "merge") %>%
    filter(!str_detect(pronunciation, "abbrev")) %>%
    mutate(
      pronunciation = str_remove_all(pronunciation, "(#|old|foreign|french|name)") %>% str_trim()
    )
  return(result)
}

# cmu <- read_cmu("http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/cmudict-0.7b")
cmu <- read_cmu("https://raw.githubusercontent.com/cmusphinx/cmudict/master/cmudict.dict", sep = " ")

cmu %>% head()
```

The authors propose to modify the CMU Dict as follows:

1. Remove non-alphabetic characters
2. Remove Phonetic stress weights like: ```AA0```, ```AH1``` -> ```AA, AH```
3. Remove acronym expansions

<!-- In order to remove non-alphabetic characters, we filter out all words that have a non-alphabetic character as the first character of the string or the first 70 entries (from careful analysis).  -->
For acronyms, CMU has provided a separate file that contains the acronym mappings, we use the mapping to remove all pronunciations for acronyms. Removing stress weights is a rather trivial task.

```{r}
## 2slow4me
check_word <- function(word) {
  result <- str_sub(word, 1, 1) %>%
    str_detect("[[:alpha:]]")
  return(result)
}

cmu_acronyms <- read_cmu("https://raw.githubusercontent.com/Alexir/CMUdict/master/acronym-0.7b") %>% mutate(word = str_to_lower(word))

fdict <- cmu %>%
  # slice(-(1:70)) %>%
  anti_join(cmu_acronyms) %>%
  mutate(
    pronunciation = str_remove_all(pronunciation, "\\d"),
    word = str_remove_all(word, "[^a-z'\\d\\(\\)]")
  ) %>%
  distinct()
```

**Mis-match #1:** The original filtered dictionary, *FDICT* contained 129,559 entries as opposed to our 134,396 entry *FDICT* in our case. Our best guess is that this is due to the version mismatch between the two CMU dicts.


encoding of pronunciation strings

```{r}
phonemes <- fdict %>% 
  select(pronunciation) %>% 
  distinct() %>% 
  unnest_tokens(phonemes, pronunciation, to_lower = F) %>% 
  distinct() %>%
  pull(phonemes)

p2e <- as.list(c(letters, LETTERS)[1:length(phonemes)])
names(p2e) <- phonemes

e2p <- as.list(phonemes)
names(e2p) <- c(letters, LETTERS)[1:length(phonemes)]

encode_cmu <- function(string) {
  #F R IH S K OW
  result <- str_split(string, " ", simplify = T) %>% 
    map_chr(~p2e[[.x]]) %>%
    glue::glue_collapse() %>%
    as.character()
  return(result)
}

fdict_encoded <- fdict %>%
  mutate(pronunciation = map_chr(pronunciation, encode_cmu))

fdict_encoded %>% sample_n(10)
```

Next, we find the words with one or more alternate pronunciations present in our *FDICT*.

```{r}
alternates <- fdict_encoded %>%
  mutate(
    alternates = str_extract(word, "\\(\\d\\)"),
    word = str_remove(word, "\\(\\d\\)")
  ) %>%
  group_by(word) %>%
  nest() %>%
  mutate(len = map_int(data, nrow)) %>%
  filter(len > 1) %>%
  select(-len) %>%
  mutate(
    data = map(data, function(x) {
      x %>% select(-alternates) %>% distinct()
    })
  ) %>%
  mutate(len = map_int(data, nrow)) %>%
  filter(len > 1) %>%
  select(-len) 
```


## String Alignment using the Modified Needleman-Wunsch Algorithm

To-do: Present motivation, computation, example.

```{r}
# string_dist("split", "plate")
# string_align("split", "plate")

tidy_align <- function(pronunciation) {
  result <- combn(pronunciation, m = 2) %>%
    t() %>%
    as_tibble() %>%
    mutate(aligned = map2(V1, V2, string_align)) %>%
    pull(aligned) %>% 
    map(function(x){
      str_split(x, "") %>% 
        bind_cols()
    }) %>% 
    bind_rows() %>% 
    rename(phoneme1 = "V1", phoneme2 = "V2")
  
  return(result)
}

test <- alternates %>%
  filter(word == "tuesday") %>%
  pull(data) %>% 
  .[[1]] %>%
  pull(pronunciation)

tidy_align(test)

substitutions <- alternates %>%
  mutate(
    substitutions = map(data, function(x) {
      return(tidy_align(x$pronunciation))
    })
  ) %>%
  select(-data) %>%
  unnest() %>%
  mutate(
    phoneme1 = map_chr(phoneme1, function(x) ifelse(x != "*", e2p[[x]], x)),
    phoneme2 = map_chr(phoneme2, function(x) ifelse(x != "*", e2p[[x]], x))
  )

aligned_phonemes <- tibble(
  phoneme = c(
    substitutions %>%
      pull(phoneme1),
    substitutions %>%
      pull(phoneme2)
  )
) %>%
  add_tally() %>%
  count(phoneme, n) %>%
  transmute(
    phoneme, 
    freq = nn/n
  )

substitutions %>%
  select(phoneme1, phoneme2) %>%
  add_count() %>%
  count(phoneme1, phoneme2, n) %>%
  transmute(phoneme1, phoneme2, freq_substitute = nn/n) %>%
  inner_join(aligned_phonemes %>% rename(freq1 = "freq", phoneme1 = "phoneme")) %>%
  inner_join(aligned_phonemes %>% rename(freq2 = "freq", phoneme2 = "phoneme")) %>%
  mutate(score = log(freq_substitute*4/(freq1 + freq2))) %>%
  select(phoneme1, phoneme2, score)
```

Possible that I am doing something wrong..
